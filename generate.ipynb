{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Sampling Example\n",
    "\n",
    "This notebook demonstrates how to use the sampling functions to generate videos from text prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageAttention loaded successfully\n",
      "flash attn 2 available False\n",
      "flash attn 3 available False\n",
      "sage attn available True\n",
      "SAGEATTN_AVAILABLE: True\n",
      "DO_COMPILE False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sample import sample_videos, sample_single_video, prompts\n",
    "from release_server import load_merge_config, load_all, compile_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 07:06:05.601 - INFO - Starting model loading...\n",
      "2025-10-20 07:06:05.603 - DEBUG - Using checkpoint: checkpoints/krea-realtime-video-14b.safetensors\n",
      "Loading transformer:   0%|          | 0/4 [00:00<?, ?it/s]2025-10-20 07:06:07.864 - DEBUG - Transformer import took: 2.26s\n",
      "2025-10-20 07:06:15.663 - DEBUG - Loading transformer state dict from checkpoints/krea-realtime-video-14b.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WAN model, with name Wan2.1-T2V-14B\n",
      "timestep shift,  5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 07:06:52.198 - DEBUG - Transformer load completed in: 44.33s, total: 46.59s\n",
      "2025-10-20 07:06:52.200 - DEBUG - Loading transformer took: 46.59s\n",
      "Loading text encoder:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:46<02:19, 46.59s/it]2025-10-20 07:06:52.203 - DEBUG - Text encoder import took: 0.00s\n",
      "2025-10-20 07:06:55.298 - DEBUG - Text encoder load completed in: 3.09s, total: 3.09s\n",
      "2025-10-20 07:06:55.300 - DEBUG - Loading text encoder took: 3.10s\n",
      "Loading VAE:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:49<00:42, 21.01s/it]         2025-10-20 07:06:55.528 - DEBUG - Using demo_utils.vae_block3.VAEEncoderWrapper\n",
      "2025-10-20 07:06:55.529 - DEBUG - Using demo_utils.vae_block3.VAEDecoderWrapper\n",
      "2025-10-20 07:06:55.561 - INFO - loading Wan-2.1/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth\n",
      "2025-10-20 07:06:56.646 - DEBUG - VAE load completed in: 1.12s\n",
      "2025-10-20 07:06:56.678 - DEBUG - Loading VAE took: 1.15s\n",
      "Initializing pipeline:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:51<00:12, 12.05s/it]2025-10-20 07:06:56.684 - DEBUG - Pipeline import took: 0.00s\n",
      "2025-10-20 07:06:56.686 - DEBUG - Pipeline initialization completed in: 0.01s\n",
      "2025-10-20 07:06:56.686 - DEBUG - Initializing pipeline took: 0.01s\n",
      "Initializing pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:51<00:00, 12.77s/it]\n",
      "2025-10-20 07:06:56.687 - INFO - All models loaded successfully in 51.08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incompatible _IncompatibleKeys(missing_keys=['mean', 'std'], unexpected_keys=[]) while loading vae decoder\n",
      "KV inference with 3 frames per block\n",
      "âœ… Models loaded and ready to use!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "config = load_merge_config(\"configs/self_forcing_server_14b.yaml\")\n",
    "models = load_all(config)\n",
    "compile_models(models)\n",
    "print(\"âœ… Models loaded and ready to use!\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling videos:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Prompt 1/1: A lone samurai in traditional armor practices kata in a quiet field at dawn, his silhouette framed against a misty horizon. Each sword strike is fluid yet forceful, the blade flashing as it slices through the air with precise arcs and sudden bursts of speed. His feet shift with practiced agility, sending up small sprays of dirt as he pivots, steps, and lunges in a continuous flow of disciplined motion. The camera rotates around the samurai, as he swings\n",
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of crossattn cache\n",
      "denoising step list:  tensor([1000.0000,  908.8427,  713.9794,    0.0000], device='cuda:0')\n",
      "ðŸŽ¬ Generating 8 blocks...\n",
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of kv cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of kv cache\n",
      "Zero reinitialization of kv cache\n",
      "âœ… Generated 90 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vast/erwann/realtime-video/.venv/lib/python3.11/site-packages/torchvision/io/_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n",
      "Sampling videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:15<00:00, 15.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Video saved to outputs/prompt_000.mp4\n",
      "\n",
      "ðŸŽ‰ All videos generated successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sample\n",
    "from sample import GenerateParams\n",
    "params = GenerateParams(\n",
    "    prompt=\"\",\n",
    "    width=832,\n",
    "    height=480,\n",
    "    num_blocks=8,\n",
    "    kv_cache_num_frames=3,\n",
    "    do_kv_recomp=True,\n",
    "    num_denoising_steps=4,\n",
    "    seed=123,\n",
    ")\n",
    "prompts = [\n",
    "    \"A lone samurai in traditional armor practices kata in a quiet field at dawn, his silhouette framed against a misty horizon. Each sword strike is fluid yet forceful, the blade flashing as it slices through the air with precise arcs and sudden bursts of speed. His feet shift with practiced agility, sending up small sprays of dirt as he pivots, steps, and lunges in a continuous flow of disciplined motion. The camera rotates around the samurai, as he swings\",\n",
    "]\n",
    "results = sample.sample_videos(\n",
    "    prompts_list=prompts,\n",
    "    models=models,\n",
    "    params=params,\n",
    "    output_dir=\"outputs\",\n",
    "    save_videos=True,\n",
    "    fps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video to Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = GenerateParams(\n",
    "    prompt=\"\",\n",
    "    width=832,\n",
    "    height=480,\n",
    "    num_blocks=9,\n",
    "    seed=1234,\n",
    "    kv_cache_num_frames=3,\n",
    "    thresh_kv_scale = 1.,\n",
    "    do_kv_recomp=True,\n",
    "    num_denoising_steps=4,\n",
    "\n",
    "    input_vid = \"https://cdn-uploads.huggingface.co/production/uploads/62a2712903bf94c3ac3ae004/UjM1G0hthNGdtItaE-qGc.mp4\" # ADD this\n",
    ")\n",
    "prompts = [\n",
    "    \"a cat knight is riding a horse across a dusty plain. the cat is a black cat wearing armor\",\n",
    "]\n",
    "results = sample.sample_videos(\n",
    "    prompts_list=[p],\n",
    "    models=models,  # Reuse loaded models\n",
    "    params=params,\n",
    "    output_dir=\"out_release\",\n",
    "    save_videos=True,\n",
    "    fps=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
