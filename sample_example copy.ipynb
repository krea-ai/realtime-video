{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Sampling Example\n",
    "\n",
    "This notebook demonstrates how to use the sampling functions to generate videos from text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "Warning: Could not load sageattention: No module named 'sageattention'\n",
      "sageattention package is not installed\n",
      "flash attn 2 available True\n",
      "flash attn 3 available False\n",
      "sage attn available False\n",
      "SAGEATTN_AVAILABLE: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# sys.path.insert(0, '/workspace/sf-copy')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msample\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sample_videos, sample_single_video, prompts\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrelease_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_merge_config, load_all\n",
      "File \u001b[0;32m/workspace/release-sf/sample.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTF\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrelease_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     load_merge_config, \n\u001b[1;32m     11\u001b[0m     load_all, \n\u001b[1;32m     12\u001b[0m     GenerateParams, \n\u001b[1;32m     13\u001b[0m     GenerationSession,\n\u001b[1;32m     14\u001b[0m     Models\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Default prompts\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/release-sf/release_server.py:51\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmsgpack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packb, unpackb\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MODEL_FOLDER\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WanVAE\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdynamo\u001b[39;00m\n\u001b[1;32m     53\u001b[0m dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrecompile_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "File \u001b[0;32m/workspace/release-sf/wan/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m configs, distributed, modules\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage2video\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WanI2V\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext2video\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WanT2V\n",
      "File \u001b[0;32m/workspace/release-sf/wan/modules/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flash_attention\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WanModel\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m T5Decoder, T5Encoder, T5EncoderModel, T5Model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingfaceTokenizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WanVAE\n",
      "File \u001b[0;32m/workspace/release-sf/wan/modules/t5.py:472\u001b[0m\n\u001b[1;32m    468\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _t5(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mumt5-xxl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mT5EncoderModel\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_len\u001b[49m\n",
      "File \u001b[0;32m/workspace/release-sf/wan/modules/t5.py:478\u001b[0m, in \u001b[0;36mT5EncoderModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mT5EncoderModel\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    476\u001b[0m         text_len,\n\u001b[1;32m    477\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[0;32m--> 478\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    479\u001b[0m         checkpoint_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    480\u001b[0m         tokenizer_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    481\u001b[0m         shard_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    482\u001b[0m     ):\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_len \u001b[38;5;241m=\u001b[39m text_len\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m dtype\n",
      "File \u001b[0;32m/workspace/release-sf/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:1071\u001b[0m, in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[0;32m/workspace/release-sf/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:412\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    411\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 412\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    416\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "import sys\n",
    "# sys.path.insert(0, '/workspace/sf-copy')\n",
    "\n",
    "from sample import sample_videos, sample_single_video, prompts\n",
    "from release_server import load_merge_config, load_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Load models once and reuse (Recommended)\n",
    "\n",
    "This approach loads the models once and reuses them for multiple prompts, which is much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_merge_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load models once\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# config = load_merge_config(\"configs/self_forcing_dmd_14b_server.yaml\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_merge_config\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/self_forcing_dmd_will_optims.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# config.checkpoint_path = \"/vast/erwann/Self-Forcing/release_candidates_14b/merged_0.1_0.0_0.9_bf16.safetensors\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m config\u001b[38;5;241m.\u001b[39mcheckpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/sf-copy/merged_checkpoints_v2/merged_0.1_0.9_0.0.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_merge_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Load models once\n",
    "# config = load_merge_config(\"configs/self_forcing_dmd_14b_server.yaml\")\n",
    "config = load_merge_config(\"configs/self_forcing_dmd_will_optims.yaml\")\n",
    "# config.checkpoint_path = \"/vast/erwann/Self-Forcing/release_candidates_14b/merged_0.1_0.0_0.9_bf16.safetensors\"\n",
    "config.checkpoint_path = \"/workspace/sf-copy/merged_checkpoints_v2/merged_0.1_0.9_0.0.safetensors\"\n",
    "\n",
    "models = load_all(config)\n",
    "\n",
    "print(\"✅ Models loaded and ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate videos for multiple prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "sd = load_file(\"/workspace/sf-copy/merged_checkpoints_v2/merged_0.1_0.9_0.0.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mgenerator\u001b[38;5;241m.\u001b[39mload_state_dict(sd,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "models.pipeline.generator.load_state_dict(sd,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sample import GenerateParams\n",
    "params = GenerateParams(\n",
    "    prompt=\"A cat playing with a ball of yarn\",\n",
    "    width=832,\n",
    "    height=480,\n",
    "    num_blocks=100,\n",
    "    is_t2v=True,\n",
    "    seed=123,\n",
    "    kv_cache_num_frames=3,\n",
    "    thresh_kv_scale = 1.,\n",
    "    do_kv_recomp=True,\n",
    "    num_denoising_steps=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sample\n",
    "importlib.reload(sample)\n",
    "p = \"A sorcerer stands with one hand outstretched, holding a roiling flame that coils and twists restlessly around his palm and fingers, sparks shooting off in unpredictable arcs. The fire spirals and lashes outward, wrapping around his arm like a living serpent before snapping back toward his hand in a continuous, fluid motion. His cloak ripples in the heat’s updraft as the flame flares and contracts, creating bursts of glowing embers that rise and scatter through the air\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = \"https://app-uploads.krea.ai/public/a8218957-1a80-43dc-81b2-da970b5f2221-video.mp4\"\n",
    "params.input_video = vid\n",
    "params.strength = .5\n",
    "params.num_blocks = 9\n",
    "results = sample.sample_videos(\n",
    "    prompts_list=[p],\n",
    "    models=models,  # Reuse loaded models\n",
    "    params=params,\n",
    "    output_dir=\"out_release\",\n",
    "    save_videos=True,\n",
    "    fps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"A sorcerer stands with one hand outstretched, holding a roiling flame that coils and twists restlessly around his palm and fingers, sparks shooting off in unpredictable arcs. The fire spirals and lashes outward, wrapping around his arm like a living serpent before snapping back toward his hand in a continuous, fluid motion. His cloak ripples in the heat’s updraft as the flame flares and contracts, creating bursts of glowing embers that rise and scatter through the air\",\n",
    "    \"Adrenaline-pumped, wide-eyed ginger kitten in vintage aviator goggles blasts down a narrow cobblestone street on a bright-yellow mini-bicycle. Camera whip-pans in from behind, then snap-zooms past the spinning front wheel into an ultra-close-up of the cat’s determined face, fur rippling in the wind. Hard sunlight streaks across the scene, creating dynamic highlights and streaking motion-blur on the spokes.\",\n",
    "    \"A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.\",\n",
    "    \"A lone samurai in traditional armor practices kata in a quiet field at dawn, his silhouette framed against a misty horizon. Each sword strike is fluid yet forceful, the blade flashing as it slices through the air with precise arcs and sudden bursts of speed. His feet shift with practiced agility, sending up small sprays of dirt as he pivots, steps, and lunges in a continuous flow of disciplined motion. The static camera holds the scene firmly in place, emphasizing the intensity and grace of his movements against the stillness of the landscape.\",\n",
    "    \"Tracking shot, cinematic style, of a surreal alien bird with iridescent feathers and bio-luminescent wings, gracefully flying through a dense alien forest — the camera follows the bird in one continuous dynamic shot, weaving through twisted glowing trees, over bioluminescent mushrooms, and past floating pollen-like orbs. The bird performs elegant aerial maneuvers — barrel rolls, sharp turns, dives — as it dodges branches and interacts with the strange atmosphere. The forest pulses with ambient light and motion-reactive flora.\",\n",
    "    \"A single crystalline drop of water hovers in midair against a soft gradient background, shimmering with refracted light. The drop quivers, rippling with internal motion, before stretching outward as if blooming from within. Its surface tension warps into delicate translucent petals that unfurl in one continuous motion. The petals radiate with glistening reflections, the drop gracefully becoming a luminous flower suspended in space.\",\n",
    "    \"Two scarred alley cats, one orange tabby with torn ears and one lean gray bruiser, stand upright in a flickering streetlight. They lash out with vicious swipes, their miniature boxing gloves cracked and worn, landing heavy blows that make fur and sweat fly. The gray cat snarls as it takes a hook to the jaw, stumbling back before lunging forward with a brutal counter. The sound of claws scraping the pavement mixes with the thud of their punches, the fight relentless and raw.\",\n",
    "    \"She returns in a radiant gown made entirely of flowing gold silk, draped in voluminous layers that trail behind her in shimmering waves. The fabric ripples dramatically with each step, catching and scattering light across the runway. The dress moves like liquid metal, transforming her walk into a continuous display of luxury and opulence, radically different from her previous sculptural look.\",\n",
    "    \"Under glowing neon lights in a surreal diner, a polar bear sits awkwardly in a red leather booth, gripping a massive BLT with mayonnaise dripping from its paws. It devours the sandwich with primal hunger, grease smearing across its white fur as the jukebox hums in the background. Fries scatter across the floor as the bear’s huge body shifts against the tiny furniture, the booth creaking under its weight. The continuous shot blends humor, absurdity, and menace in equal measure.\",\n",
    "    \"Inside a steaming porcelain coffee cup, two tiny wooden galleons clash on dark, swirling waves of coffee. Cannons fire with sharp flashes, sending miniature plumes of smoke curling into the air, while splinters of wood spray from direct hits. The surface of the coffee sloshes violently with every broadside, waves lapping against the cup’s rim. Steam drifts upward, blending with the smoke, as the ships circle each other in a chaotic, continuous duel.\",\n",
    "    \"In a brightly lit gymnasium, a young gymnast in a red leotard swings powerfully on the uneven bar, launching into a perfect backflip. Her body tucks tightly mid-air, spinning in one continuous motion before her hands snap back onto the bar with precision. Chalk dust bursts into the air with the impact, drifting like smoke around her. The crowd gasps faintly in the background, the scene locked in graceful, dynamic motion.\",\n",
    "    \"Under the glare of bright stadium lights, the golden retriever launches high off the springboard, twisting into a daring double flip. Its ears flap wildly mid-air, and the shimmering pool reflects its spinning form. The audience erupts in cheers as the dog’s paws extend gracefully in the final moment before the dive. A splash explodes upward as it enters the water, sending ripples across the Olympic rings painted on the pool floor.\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_prompts[:2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sample\n",
    "importlib.reload(sample)\n",
    "\n",
    "import sample\n",
    "from safetensors.torch import load_file\n",
    "import glob\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Sample videos for all prompts with custom settings\n",
    "\n",
    "model_paths = glob.glob(\"merged_checkpoints/*\")\n",
    "print(\"model paths:\", model_paths)\n",
    "\n",
    "\n",
    "\n",
    "for i, model_path in enumerate(model_paths):\n",
    "    print(\"loading model\", model_path)\n",
    "    sd = load_file(model_path)\n",
    "    sd = {\n",
    "        \"model.\" + k if not k.startswith(\"model.\") else k: v for k, v in sd.items()\n",
    "    }\n",
    "    m, u = models.pipeline.generator.load_state_dict(sd, strict=False)\n",
    "    for block in models.pipeline.generator.model.blocks:\n",
    "        block.self_attn.fused_projections = False\n",
    "    \n",
    "    # print(\"model loaded, missing:\", m, \"unexpected:\", u)\n",
    "    del sd\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    output_dir = f\"merge_sweep_outputs/{i}_{os.path.basename(model_path)}\"\n",
    "    results = sample.sample_videos(\n",
    "        prompts_list=test_prompts,\n",
    "        models=models,  # Reuse loaded models\n",
    "        params=params,\n",
    "        output_dir=output_dir,\n",
    "        save_videos=True,\n",
    "        fps=16\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Display results\n",
    "for idx, result in results.items():\n",
    "    print(f\"\\nPrompt {idx}: {result['prompt']}\")\n",
    "    print(f\"  Frames: {result['num_frames']}\")\n",
    "    print(f\"  Video: {result['video_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import wan\n",
    "import wan.modules.causal_model\n",
    "importlib.reload(wan.modules.causal_model)\n",
    "import wan.modules.causal_model\n",
    "# from wan.modules.causal_model import *\n",
    "\n",
    "dim = models.pipeline.generator.model.dim \n",
    "num_heads = models.pipeline.generator.model.num_heads\n",
    "d = dim // num_heads\n",
    "models.pipeline.generator.model.freqs = torch.cat([\n",
    "    # rope_params(1024, d - 4 * (d // 6)),\n",
    "    wan.modules.causal_model.rope_params_riflex(1024, d - 4 * (d // 6), k=6, L_test=90),\n",
    "    rope_params(1024, 2 * (d // 6)),\n",
    "    rope_params(1024, 2 * (d // 6))\n",
    "],\n",
    "    dim=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sample\n",
    "params.num_blocks = 9 \n",
    "results = sample.sample_videos(\n",
    "    prompts_list=test_prompts[:1],\n",
    "    models=models,  # Reuse loaded models\n",
    "    params=params,\n",
    "    output_dir=\"riflex\",\n",
    "    save_videos=True,\n",
    "    fps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a single video\n",
    "video_path = sample_single_video(\n",
    "    prompt=\"A cat playing with a ball of yarn\",\n",
    "    models=models,  # Reuse loaded models\n",
    "    num_blocks=15,\n",
    "    width=832,\n",
    "    height=480,\n",
    "    seed=123,\n",
    "    output_path=\"outputs/cat_video.mp4\"\n",
    ")\n",
    "\n",
    "print(f\"Video saved to: {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom prompts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom prompts\n",
    "custom_prompts = [\n",
    "    \"A dog running on a beach at sunset\",\n",
    "    \"A city skyline at night with lights\",\n",
    "    \"A waterfall in a lush forest\",\n",
    "]\n",
    "\n",
    "# Generate videos\n",
    "results = sample_videos(\n",
    "    prompts_list=custom_prompts,\n",
    "    models=models,  # Reuse loaded models\n",
    "    num_blocks=15,\n",
    "    is_t2v=True,\n",
    "    output_dir=\"outputs/custom_samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Auto-load models (simpler but slower for multiple calls)\n",
    "\n",
    "If you don't pass the `models` parameter, they will be loaded automatically (but this loads them fresh each time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will load models automatically\n",
    "video_path = sample_single_video(\n",
    "    prompt=\"A person riding a bicycle\",\n",
    "    num_blocks=1,\n",
    "    output_path=\"outputs/bicycle_video.mp4\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
